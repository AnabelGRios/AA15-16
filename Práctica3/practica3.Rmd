---
title: "Práctica 3"
author: "Anabel Gómez Ríos"
output: pdf_document
---
```{r}
library(ISLR)
#library(MASS)
#library(class) # Para el KNN
# Para ahorrarnos el prefijo en Auto$mpg (cada vez que queramos acceder a algo de
# Auto: 
attach(Auto)
```

#Ejercicio 1
#Usar el conjunto de datos Auto que es parte del paquete ISLR. En este ejercicio desarrollaremos un modelo para predecir si un coche tiene un consumo de carburante alto o bajo usando la base de datos Auto. Se considerará alto cuando sea superior a la media de la variable mpg y bajo en caso contrario.

##a) Usar las funciones de R pairs() y boxplot() para investigar la dependencia entre mpg y las otras características. ¿Cuáles de las otras características parece más útil para predecir mpg? Justificar la respuesta.

```{r}
pairs(Auto)
```

Como vemos con la función `pairs`, las características que parecen más útiles para predecir mpg (que son aquellas que tienen un patrón más o menos claro con respecto a mpg) son `displacement`, `horsepower` y `weight`.  

Vamos a ver ahora las tres seleccionadas con `boxplot`:
```{r}
boxplot(Auto$mpg~Auto$cylinders)
boxplot(Auto$mpg~Auto$horsepower)
boxplot(Auto$mpg~Auto$weight)
```

Como vemos las tres siguen más o menos una estructura clara. Veamos otra que no sea ninguna de estas tres:
```{r}
boxplot(Auto$mpg~Auto$name)
boxplot(Auto$mpg~Auto$acceleration)
```
Estas por ejemplo, vemos que no tienen mucho que ver con `mpg`.

##b) Seleccionar las variables predictorias que considere más relevantes.

Lo que vamos a hacer es crear otro `data.frame` en el que vamos a eliminar el resto de variables, y vamos a dejar sólo las que hemos citado previamente.
```{r}
# Elegimos las cinco primeras columnas de Auto, que contienen mpg, cylinders,
# displacement, horsepower y weight
AutoMod <- Auto[ ,1:5]
# Nos sobra la columna de cylinders, así que la eliminamos
AutoMod <- AutoMod[ ,-2]
```

##c) Particionar el conjunto de datos en un conjunto de entrenamiento (80%) y otro de test (20%). Justificar el procedimiento usado.

```{r}
train = sample(1:nrow(AutoMod), round(nrow(AutoMod)*0.8))
test = AutoMod[-train, ]
train = AutoMod[train, ]
```

##d) Crear una variable binaria, mpg01, que será igual a 1 si la variable mpg contiene un valor por encima de la mediana, y -1 si mpg contiene un valor por debajo de la mediana. La mediana se puede calcular usando la función median(). (Nota: puede resultar útil usar la función data.frames() para unir en un mismo conjunto de datos la nueva variable mpg01 y las otras variables de Auto).

```{r}
posiciones <- c(1:length(train[,1]))
pos_positivos <- posiciones[train[,1] >= median(train[,1])]
pos_negativos <- posiciones[train[,1] < median(train[,1])]
train[,1][pos_positivos] = 1
train[,1][pos_negativos] = -1

posiciones <- c(1:length(test[,1]))
pos_positivos <- posiciones[test[,1] >= median(test[,1])]
pos_negativos <- posiciones[test[,1] < median(test[,1])]
test[,1][pos_positivos] = 1
test[,1][pos_negativos] = -1
```


###1. Ajustar un modelo de regresión logística a los datos de entrenamiento y predecir mpg01 usando las variables seleccionadas en b). ¿Cuál es el error de test del modelo? Justificar la respuesta.

```{r}
modlog1 <- glm(train$mpg~., data = train)
predicciones <- predict(modlog1, test)
comp <- sign(predicciones) == sign(test$mpg)
errores <- comp[comp == FALSE]
error.regresion <- 100*(length(errores)/nrow(test))
cat("El error con regresión ha sido:", error.regresion)
```


###2. Ajustar un modelo K-NN a los datos de entrenamiento y predecir mpg01 usando solamente las variables seleccionadas en b). ¿Cuál es el error de test en el modelo? ¿Cuál es el valor de K que mejor ajusta los datos? Justificar la respuesta. (Usar el paquete class de R).


```{r}
escalado <- scale(train[,2:4])
train[,2:4] <- escalado
centro <- attr(escalado,"scaled:center")
escala <- attr(escalado, "scaled:scale")
test[,2:4] <- scale(test[,2:4], center=centro, scale=escala)
```


```{r}
library(class)
library(e1071)
# Pasamos los datos con los que vamos a predecir a una matriz
x <- as.matrix(train[,-1])
# Fijamos la semilla
set.seed(237)
tune.knn(x,as.factor(train[,1]), k=1:10, tunecontrol=tune.control(sampling = "cross"))
# Utilizamos knn con el mejor k que nos ha dicho tune.knn
pred.knn <- knn(train[,-1], test[,-1], train[,1], k=5)

```

Vamos  a ver ahora el error de test
```{r}
confM <- table(pred.knn, test[,1])
error <- (confM[1,2] + confM[2,1])/(confM[1,1]+confM[1,2]+confM[2,1]+confM[2,2])
```

###3. Pintar las curvas ROC (instalar paquete ROCR de R) y comparar y valorar los resultados obtenidos para ambos modelos.

```{r}
library(ROCR)
knn.res <- knn(train[,-1], test[,-1], train[,1], k=5, prob=T)
prob <- attr(knn.res,"prob")
prob <- sapply(1:length(prob), function(i) {
  if(as.numeric(knn.res[i]) == 1) {
    1-prob[i]
  }
  else {
    prob[i]
  }
 })

pred <- prediction(prob, test$mpg)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
```


##e) Bonus-1: Estimar el error de test de ambos modelos (RL, k-NN) pero usando Validación Cruzada de 5-particiones. Comparar con los resultados obtenidos en el punto anterior.

##f) Bonus-2: Ajustar el mejor modelo de regresión posible considerando la variable mpg como salida y el resto como predictorias. Justificar el modelo ajustado en base al patrón de los residuos. Estimar su error de entrenamiento y test.

```{r}
# Borramos lo que no necesitamos
rm(AutoMod, escalado, test, train, x)
rm(centro, comp, confM, error, error.regresion, errores)
rm(escala, knn.res, modlog1, perf, pos_negativos, pos_positivos)
rm(posiciones, pred, pred.knn, predicciones, prob)
```


#Ejercicio 2.
#Usar la base de datos Boston (en el paquete MASS de R) para ajustar un modelo que prediga si dado un suburbio este tiene una tasa de criminalidad (crim) por encima o por debajo de la mediana. Para ello considere la variable crim como la variable salida y el resto como variables predictoras.

```{r}
library(MASS)
# Dividimos el conjunto en train (80%) y test(20%)
train = sample(1:nrow(Boston), round(nrow(Boston)*0.8))
test = Boston[-train, ]
train = Boston[train, ]
```


##a) Encontrar el subconjunto óptimo de variables predictoras a partir de un modelo de regresión-LASSO (usar paquete glmnet de R) donde seleccionamos sólo aquellas variables con coeficiente mayor de un umbral prefijado.

Utilizando `glmnet`, le tenemos que dar un 1 al parámetro `alpha` para que sea un modelo LASSO.

```{r}
library(glmnet)
# Volvemos a fijar la semilla
set.seed(237)
modelo.lasso <- glmnet(as.matrix(train[,-1]), as.matrix(train[,1]), alpha=1)
plot(modelo.lasso)
```

Como vemos, muchos de los coeficientes están cerca de cero, o son exactamente cero, lo que quiere decir que las variables correspondientes no influirán en la variable `crim`. Vamos a elegir un $\lambda$ apropiado usando cross-validation.

```{r}
crossv <- cv.glmnet(as.matrix(train[,-1]), as.matrix(train[,1]), alpha=1)
lambda <- crossv$lambda.min
coeficientes <- predict(modelo.lasso, type="coefficients", s=lambda)[1:14,]
print(coeficientes)
```

Como vemos hay muchos de estos coeficientes muy cercanos a 0. Nos vamos a quedar con aquellos que estén (en valor absoluto) por encima de un umbral 0.5, que significará que tienen cierta correlación con la variable `crim`, aunque podríamos elegir uno un poco más bajo si quisiéramos considerar más variables (aunque serían menos relevantes)

```{r}
coeficientes <- coeficientes[abs(coeficientes)>0.5]
print(coeficientes)
```

Nos quedamos sólo entonces con las variables `chas`, `nox`, `dis` y `rad`.

##b) Ajustar un modelo de regresión regularizada con "weight-decay" (ridge-regression) y las variables seleccionadas. Estimar el error residual del modelo y discutir si el comportamiento de los residuos muestran algún indicio de "underfitting".

Para $\textit{weight-decay}$ tenemos que usar también `glmnet` pero con el parámetro $\alpha=0$.  
Nos quedamos primero con las variables seleccionadas por el método anterior, y vamos a utilizar el mejor $\lambda$ que nos ha salido del apartado anterior por cross-validation:

```{r}
BostonMod.train <- data.frame(train$crim, train$chas, train$nox, train$dis,
                              train$rad)
BostonMod.test <- data.frame(test$crim, test$chas, test$nox, test$dis, test$rad)
modelo.ridge <- glmnet(as.matrix(BostonMod.train[,-1]),
                       as.matrix(BostonMod.train[,1]), alpha=0, lambda=lambda)
predicciones <- predict(modelo.ridge, s=lambda,
                        newx=as.matrix(BostonMod.test[,-1]))
```


Calculamos ahora el error residual, que será la raíz cuadrada positiva de los cuadrados de las diferencias entre nuestro valor y el predicho por el modelo.

```{r}
error.res <- sum((BostonMod.test[,1] - predicciones)^2)
error.res <- sqrt(error.res/length(predicciones))
```

Para ver ahora si estamos o no ajustando poco el modelo (underfitting) vamos a probar distintos valores de $\lambda$, que es el parámetro que maneja la cantidad de regularización que le damos al modelo. Vamos a coger dos valores por debajo del $\lambda$ que tenemos en este momento (0.0450578) y dos por encima y comprobar qué sucede:

```{r}
lambda <- 0.08
modelo.ridge <- glmnet(as.matrix(BostonMod.train[,-1]),
                       as.matrix(BostonMod.train[,1]), alpha=0, lambda=lambda)
predicciones <- predict(modelo.ridge, s=lambda,
                        newx=as.matrix(BostonMod.test[,-1]))
error.res <- sum((BostonMod.test[,1] - predicciones)^2)
error.res <- sqrt(error.res/length(predicciones))
cat("El error para lambda =", lambda, "ha sido", error.res)
```
```{r}
lambda <- 1
modelo.ridge <- glmnet(as.matrix(BostonMod.train[,-1]),
                       as.matrix(BostonMod.train[,1]), alpha=0, lambda=lambda)
predicciones <- predict(modelo.ridge, s=lambda,
                        newx=as.matrix(BostonMod.test[,-1]))
error.res <- sum((BostonMod.test[,1] - predicciones)^2)
error.res <- sqrt(error.res/length(predicciones))
cat("El error para lambda =", lambda, "ha sido", error.res)
```
```{r}
lambda <- 0.02
modelo.ridge <- glmnet(as.matrix(BostonMod.train[,-1]),
                       as.matrix(BostonMod.train[,1]), alpha=0, lambda=lambda)
predicciones <- predict(modelo.ridge, s=lambda,
                        newx=as.matrix(BostonMod.test[,-1]))
error.res <- sum((BostonMod.test[,1] - predicciones)^2)
error.res <- sqrt(error.res/length(predicciones))
cat("El error para lambda =", lambda, "ha sido", error.res)
```
```{r}
lambda <- 0.002
modelo.ridge <- glmnet(as.matrix(BostonMod.train[,-1]),
                       as.matrix(BostonMod.train[,1]), alpha=0, lambda=lambda)
predicciones <- predict(modelo.ridge, s=lambda,
                        newx=as.matrix(BostonMod.test[,-1]))
error.res <- sum((BostonMod.test[,1] - predicciones)^2)
error.res <- sqrt(error.res/length(predicciones))
cat("El error para lambda =", lambda, "ha sido", error.res)
```

Como vemos, cuanto mayor es el $\lambda$, menor es el error (mayor la cantidad de regularización), mientras que si lo bajamos, el error aumenta, por lo que sí que estamos ajustando poco el modelo aún, se podría mejorar aumentando $\lambda$.

##c) Definir una nueva variable con valores -1 y 1 usando el valor de la mediana de la variable crim como umbral. Ajustar un modelo SVM que prediga la nueva variable definida. (Usar el paquete e1071 de R). Describir con detalle cada uno de los pasos dados en el aprendizaje del modelo SVM. Comience ajustando un modelo lineal y argumente si considera necesario usar algún núcleo. Valorar los resultados del uso de distintos núcleos.

```{r}
posiciones <- c(1:length(train[,1]))
pos_positivos <- posiciones[train[,1] >= median(train[,1])]
pos_negativos <- posiciones[train[,1] < median(train[,1])]
train[,1][pos_positivos] = 1
train[,1][pos_negativos] = -1

posiciones <- c(1:length(test[,1]))
pos_positivos <- posiciones[test[,1] >= median(test[,1])]
pos_negativos <- posiciones[test[,1] < median(test[,1])]
test[,1][pos_positivos] = 1
test[,1][pos_negativos] = -1
```

Vamos a hacer ahora un SVM con núcleo lineal

```{r}
# Volvemos a fijar la semilla
set.seed(237)
modelo.svm <- svm(train[,1]~., train[,-1], kernel="linear")
predicciones <- predict(modelo.svm, test[,-1])
comp <- sign(predicciones) == sign(test$crim)
errores <- comp[comp == FALSE]
error <- 100*(length(errores)/nrow(test))
cat("El error con SVM lineal ha sido:", error)
```

Vamos a probar con los otros tres tipos de núcleos a ver los resultados en errores que obtenemos

```{r}
modelo.svm <- svm(train[,1]~., train[,-1], kernel="polynomial")
predicciones <- predict(modelo.svm, test[,-1])
comp <- sign(predicciones) == sign(test$crim)
errores <- comp[comp == FALSE]
error <- 100*(length(errores)/nrow(test))
cat("El error con SVM polinomial ha sido:", error)
```
```{r}
modelo.svm <- svm(train[,1]~., train[,-1], kernel="radial")
predicciones <- predict(modelo.svm, test[,-1])
comp <- sign(predicciones) == sign(test$crim)
errores <- comp[comp == FALSE]
error <- 100*(length(errores)/nrow(test))
cat("El error con SVM radial ha sido:", error)
```
```{r}
modelo.svm <- svm(train[,1]~., train[,-1], kernel="sigmoid")
predicciones <- predict(modelo.svm, test[,-1])
comp <- sign(predicciones) == sign(test$crim)
errores <- comp[comp == FALSE]
error <- 100*(length(errores)/nrow(test))
cat("El error con SVM sigmoidal ha sido:", error)
```


##Bonus-3: Estimar el error de entrenamiento y test por validación cruzada de 5 particiones.

```{r}
# Borramos lo que no necesitamos
rm(BostonMod.test, BostonMod.train, test, train)
rm(coeficientes, comp, crossv, error, error.res, errores)
rm(lambda, modelo.lasso, modelo.ridge, predicciones, modelo.svm)
```


#Ejercicio 3.
#Usar el conjunto de datos Boston y las librerías randomForest y gbm de R.

##a) Dividir la base de datos en dos conjuntos de entrenamiento (80%) y test (20%).

##b) Usando la variable medv como salida y el resto como predictoras, ajustar un modelo de regresión usando bagging. Explicar cada uno de los parámetros usados. Calcular el error del test.

##c) Ajustar un modelo de regresión usando Random Forest. Obtener una estimación del número de árboles necesario. Justificar el resto de parámetros usados en el ajuste. Calcular el error de test y compararlo con el obtenido con bagging.

##d) Ajustar un modelo de regresión usando Boosting (usar gbm con distribution = 'gaussian'). Calcular el error de test y compararlo con el obtenido con bagging y Random Forest.


#Ejercicio 4.
#Usar el conjunto de datos OJ que es parte del paquete ISLR.

##a) Crear un conjunto de entrenamiento conteniendo una muestra aleatoria de 800 observaciones, y un conjunto de test conteniendo el resto de las observaciones. Ajustar un árbol a los datos de entrenamiento, con Purchase como la variable respuesta y las otras variables como predictoras (paquete tree de R).

```{r}
# Fijamos de nuevo la semilla
set.seed(237)

library(ISLR)
train.idx = sample(1:nrow(OJ), 800)
test = OJ[-train.idx, ]
train = OJ[train.idx, ]

# Usamos la librería tree
library(tree)
```

Para saber si usar clasificación o regresión vamos a ver de qué tipo es la variable `Purchase` con la función `summary()`

```{r}
summary(OJ)
```

Como vemos, `Purchase` toma sólo dos valores: `CH` y `MM`, con lo que vamos a utilizar clasificación:

```{r}
tree.oj <- tree(train$Purchase~., train)
```

Vamos a ver el resultado de este árbol:
```{r}
plot(tree.oj)
text(tree.oj, pretty = 0)
```

Como este no queda muy bien, vamos a hacerlo también con el software `RWeka`, que también permite dibujar árboles, y se ven mejor:

```{r}
library(RWeka)
library(partykit)
oj.rweka = J48(OJ$Purchase~., data = OJ, subset = train.idx)
plot(oj.rweka)
```


##c) Usar la función summary() para generar un resumen estadístico acerca del árbol y describir los resultados obtenidos: tasa de error de training, número de nodos del árbol, etc.

```{r}
summary(tree.oj)
```

Como vemos, el número de nodos hoja son 9. El número total de nodos lo podemos ver en el árbol anterior y es 17. Las variables que se han utilizado en los nodos internos (aquellos que no son hoja) nos lo dice también la función `summary()` y son `LoyalCH`, `SalePriceMM`, `PriceDiff`, `ListPriceDiff` y `DiscCH`.  
También nos dice el error en training: 0.1638 (16%) y la desviación residual hasta la media, que en este caso es la desviación normal a la media entre 800 (total de instancias en train) - 6 (número de nodos hoja) = 794, lo que da 0.7303. Como es lógico, a menor desviación, mejor ajusta el árbol a los datos de train.

##d) Predecir la respuesta de los datos de test, y generar e interpretar la matriz de confusión de los datos de test. ¿Cuál es la tasa de error del test? ¿Cuál es la precisión del test?

Para esto podemos usar de nuevo la función `predict()` pasándole el árbol que hemos entrenado con train y los datos de test. Le ponemos el argumento `type=class` porque estamos con un árbol de clasificación y así obligamos a utilziar la predicción con la variable `Purchase`. Para calcular el error de test y su precisión vamos a utilizar la función `table()`, que nos devuelve la matriz de confusión.

```{r}
tree.predict <- predict(tree.oj, test[,-1], type="class")
table(tree.predict, test[,1])
```

El error en test es (11+39)/(125+11+39+95) = 0.1851852  
La precisión es (125+95)/(125+11+39+95) = 0.8148148  
Es decir, hay un error en test del 18.5% y una precisión del 81.4%.

##e) Aplicar la función cv.tree() al conjunto de training y determinar el tamaño óptimo del árbol. ¿Qué hace cv.tree?

La misión de `cv.tree()` es utilizar cross-validation para obtener el mejor nivel de complejidad para el árbol que se obtiene. Este "mejor nivel" se puede obtener en base a diferentes criterior. Por ejemplo, si usamos `cv.tree()` con los parámetros por defecto, nos devolverá aquel que tenga menor desvianza. Si queremos que nos devuelva aquel que tenga menor error en la validación cruzada tenemos que usar el parámetro `FUN = prune.misclass`.

```{r}
set.seed(237)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
print(cv.oj)
```

Como podemos ver, los árboles con 9 y 5 nodos terminales son los que tienen el menor error, 144. El árbol que habíamos ajustado en el apartado c) tenía justo 9 nodos terminales, por lo que ya habíamos obtenido aquel con menor error y mejor precisión.

##Bonus-4. Generar un gráfico con el tamaño del árbol en el eje x (número de nodos) y la tasa de error de validación cruzada en el eje y. ¿Qué tamaño de árbol corresponde a la tasa más pequeña de error de clasificación por validación cruzada?